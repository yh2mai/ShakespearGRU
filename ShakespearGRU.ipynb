{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79bce231-cb01-4085-91ea-3a194b263763",
   "metadata": {},
   "source": [
    "# Shakespear Char prediction using GRU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c0b8741-73be-49b5-b463-16f99d4f7b93",
   "metadata": {},
   "source": [
    "This notebook walk through how to use GRU to write Shakespear style text. The content is roughtly like below:\n",
    "1. Import libraries needed\n",
    "2. Load data from file, create a char to index map and an index to char map\n",
    "3. Create a GRU class for Char\n",
    "4. Train the GRU using data loaded and generated Shakespear style text during training to see how the model performs during training\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fbadf-78aa-4e7d-b24f-f98f7cd01177",
   "metadata": {},
   "source": [
    "# 1. Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38730c6-2ab8-4d75-b7ac-51340f88b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d746e-959f-441b-9b46-a56c1a88af9d",
   "metadata": {},
   "source": [
    "# 2. Load data from file, create a char to index map and an index to char map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f8aab6-e49d-4675-a0cb-c4f4993e4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load data\n",
    "# -------------------------------\n",
    "df = pd.read_csv('data/Shakespeare_data.csv')\n",
    "data = ' '.join(df['PlayerLine'].astype(str))\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ef130-7eb6-45df-ac41-45a673a3cf59",
   "metadata": {},
   "source": [
    "# 3. Create a GRU class for Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104bd602-c511-4cdf-88f6-3db44df45e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Model definition\n",
    "# -------------------------------\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size)  # one-hot embedding\n",
    "        self.gru = nn.GRU(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)  # [batch, seq, vocab_size]\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out)  # [batch, seq, vocab_size]\n",
    "        return out, hidden\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "hidden_size = vocab_size\n",
    "seq_length = 25\n",
    "lr = 1e-2\n",
    "num_iters = 10000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CharGRU(vocab_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2330b-c5aa-40b0-bdc9-b7e9ccaffe0a",
   "metadata": {},
   "source": [
    "# 4. Train the GRU using data loaded and generated Shakespear style text during training to see how the model performs during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d265df51-dbfd-4160-917d-787983cd8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss 4.2934\n",
      "----\n",
      "Awy8XbRGJ[KBG$YCL,y7NJA8.pm-lKu57eerlzTy$k?u2]JEnYoSpe0a8InGgnHflYM.oGE6'pdOVFp,EhdJbi1tVTf.5,!]W.2OV'RkJdQakX9[IhLMowUh?X98G!w]I]m:l?4TBsw3xRA$y02]99[)xkBiILEeeTJB.\tG'bZor.tiDfLq3c II.d85SnqyfDH5-Xqr8\n",
      "----\n",
      "Iter 1000, Loss 2.7859\n",
      "----\n",
      "untime, Whe yor you cmammy, I day stlals nortuntne. Yous doind yoortelfsh I divis held grue to gcord stit if On is do,, truee yoncesur, I wagl hal, Not to daSs sciis Detoll ut bak dond sco ul dour cizp\n",
      "----\n",
      "Iter 2000, Loss 2.1692\n",
      "----\n",
      "t tes no ti. But in am in even, heven no seve hon! 'whe croleking four in frts le elon wite heis coldar me il ame hele ceirs fare for in theim four kintrink ens in? pest thee and in buck? alde me mas i\n",
      "----\n",
      "Iter 3000, Loss 1.8237\n",
      "----\n",
      "h too dooth! A ave sandy a sucwlnberoothiald me sit arty inrots I'llroth-e, a sook, and In piret me winks your trueming and sitt't shet and the if, and 'to no thist tiven Ann your eave shearthe, ye of \n",
      "----\n",
      "Iter 4000, Loss 3.7332\n",
      "----\n",
      "Ey ber whore ant sopin thelet't tyetFTLAFF, tALt ye I an onnre gow noy liphere my so sutery, and, theouttes Seem on trut? themk, therpove thet the in an nin thermow shetlcomnresbestothe truch, thin the\n",
      "----\n",
      "Iter 5000, Loss 2.0603\n",
      "----\n",
      "to, fell woor for hont me, FALSTAFFF bunt FALASMAFFFFFFFAFFFF , And hasetl! DOUGLASERRMOFOTASGLAFULAST, Eses HENASntTut whosp wongll deid thlall! hoced an HOT HOFAFSTFERERLALOSTAFFFFAFF O, If HOTghonke\n",
      "----\n",
      "Iter 6000, Loss 2.1224\n",
      "----\n",
      " no's blould wlast theare, wodbiswe knood, tuales copce les tay-Phaus hence swiy ouef meth, Haeres anon sucly searle wee. Goode, with in, an, wacrlek, ou himmeinbe. I wed weagainVull hevesepes here in \n",
      "----\n",
      "Iter 7000, Loss 1.7887\n",
      "----\n",
      "uckasew it the, Puwencen Fod Gracred wruce wot, fliw lowdit hesterntevell the touiser, Bacce, rungtad, Somb tolust al'd off to then oromave rokru tosre This Greay Morse pllendared, Or sro not Tise tolc\n",
      "----\n",
      "Iter 8000, Loss 2.0778\n",
      "----\n",
      "thience with prtumen reuy Weound hous thy Freat thran'dseentruntry: prings, 'nd in, shemem jouged nok, sphmas dy sell witil with freest in wion, Ang Gout. I the the with preawideys, net frive fosfurlhe\n",
      "----\n",
      "Iter 9000, Loss 3.5529\n",
      "----\n",
      " adites, I war dear filerd bthe Camnom, Pome crom leay. CALeath hiin mm! deallo,, My leake is, fith dealde sie. Os andsp, My fo, the death hake hajame andouldilfe, I, where, Mme, ase, I ath both, Feabb\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def get_batch(p, seq_length):\n",
    "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).unsqueeze(0)\n",
    "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).unsqueeze(0)\n",
    "    return inputs.to(device), targets.to(device)\n",
    "\n",
    "\n",
    "def sample(model, start_ix, length=200):\n",
    "    model.eval()\n",
    "    ixes = [start_ix]\n",
    "    input = torch.tensor([[start_ix]], dtype=torch.long).to(device)\n",
    "    hidden = None\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        probs = torch.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy().ravel()\n",
    "        ix = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "        ixes.append(ix)\n",
    "        input = torch.tensor([[ix]], dtype=torch.long).to(device)\n",
    "    return ''.join(ix_to_char[i] for i in ixes)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def get_batch(p, seq_length):\n",
    "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).unsqueeze(0)\n",
    "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).unsqueeze(0)\n",
    "    return inputs.to(device), targets.to(device)\n",
    "\n",
    "\n",
    "def sample(model, start_ix, length=200):\n",
    "    model.eval()\n",
    "    ixes = [start_ix]\n",
    "    input = torch.tensor([[start_ix]], dtype=torch.long).to(device)\n",
    "    hidden = None\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        probs = torch.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy().ravel()\n",
    "        ix = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "        ixes.append(ix)\n",
    "        input = torch.tensor([[ix]], dtype=torch.long).to(device)\n",
    "    return ''.join(ix_to_char[i] for i in ixes)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "p = 0\n",
    "for n in range(num_iters):\n",
    "    if p + seq_length + 1 >= len(data):\n",
    "        p = 0\n",
    "\n",
    "    inputs, targets = get_batch(p, seq_length)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _ = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs.squeeze(0), targets.squeeze(0))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print(f\"Iter {n}, Loss {loss.item():.4f}\")\n",
    "        sample_ix = sample(model, inputs[0, 0].item(), 200)\n",
    "        print(\"----\\n\" + sample_ix + \"\\n----\")\n",
    "        model.train()  # <--- ADD THIS\n",
    "\n",
    "    p += seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636eafd-68a8-4f88-9536-b453959191b2",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15dab335-8b90-4c64-ac5f-65eecefaeefd",
   "metadata": {},
   "source": [
    "The model generates random characters at the begining of the training process. It generates more reasonable characters after more training. The loss is  reducing at the begining and reach 1-3 eventually. I believe it generate better result if it is a word GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06d7ed-6152-4f33-b46d-902597b7434d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
